{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51118a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下載 nltk 資源（只需一次）\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "35fa7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = datasets.load_from_disk(\"super-emotion\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "\n",
    "texts = train_dataset[\"text\"]\n",
    "labels = train_dataset[\"labels_str\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44aa72",
   "metadata": {},
   "source": [
    "### Data Preprocessing(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "#RAM會爆炸!!\n",
    "texts_cleaned = [preprocess_text(t) for t in texts[:50000]]\n",
    "labels_subset = labels[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7c0553fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(texts_cleaned).toarray().astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f5a6e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(labels_subset)   # shape = (n_samples, n_emotions)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test = torch.tensor(y_test, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8652a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "71bb329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/75], Loss: 1.9475, Acc: 0.1781\n",
      "Epoch [2/75], Loss: 1.9452, Acc: 0.2415\n",
      "Epoch [3/75], Loss: 1.9426, Acc: 0.2735\n",
      "Epoch [4/75], Loss: 1.9397, Acc: 0.2898\n",
      "Epoch [5/75], Loss: 1.9365, Acc: 0.2991\n",
      "Epoch [6/75], Loss: 1.9328, Acc: 0.3049\n",
      "Epoch [7/75], Loss: 1.9289, Acc: 0.3083\n",
      "Epoch [8/75], Loss: 1.9247, Acc: 0.3158\n",
      "Epoch [9/75], Loss: 1.9202, Acc: 0.3298\n",
      "Epoch [10/75], Loss: 1.9154, Acc: 0.3486\n",
      "Epoch [11/75], Loss: 1.9105, Acc: 0.3725\n",
      "Epoch [12/75], Loss: 1.9053, Acc: 0.3996\n",
      "Epoch [13/75], Loss: 1.9000, Acc: 0.4228\n",
      "Epoch [14/75], Loss: 1.8943, Acc: 0.4469\n",
      "Epoch [15/75], Loss: 1.8885, Acc: 0.4783\n",
      "Epoch [16/75], Loss: 1.8825, Acc: 0.5105\n",
      "Epoch [17/75], Loss: 1.8764, Acc: 0.5429\n",
      "Epoch [18/75], Loss: 1.8700, Acc: 0.5763\n",
      "Epoch [19/75], Loss: 1.8633, Acc: 0.6038\n",
      "Epoch [20/75], Loss: 1.8567, Acc: 0.6281\n",
      "Epoch [21/75], Loss: 1.8494, Acc: 0.6506\n",
      "Epoch [22/75], Loss: 1.8423, Acc: 0.6663\n",
      "Epoch [23/75], Loss: 1.8347, Acc: 0.6812\n",
      "Epoch [24/75], Loss: 1.8274, Acc: 0.6917\n",
      "Epoch [25/75], Loss: 1.8194, Acc: 0.7007\n",
      "Epoch [26/75], Loss: 1.8114, Acc: 0.7064\n",
      "Epoch [27/75], Loss: 1.8031, Acc: 0.7145\n",
      "Epoch [28/75], Loss: 1.7945, Acc: 0.7187\n",
      "Epoch [29/75], Loss: 1.7857, Acc: 0.7258\n",
      "Epoch [30/75], Loss: 1.7764, Acc: 0.7284\n",
      "Epoch [31/75], Loss: 1.7675, Acc: 0.7301\n",
      "Epoch [32/75], Loss: 1.7583, Acc: 0.7331\n",
      "Epoch [33/75], Loss: 1.7483, Acc: 0.7360\n",
      "Epoch [34/75], Loss: 1.7385, Acc: 0.7383\n",
      "Epoch [35/75], Loss: 1.7284, Acc: 0.7413\n",
      "Epoch [36/75], Loss: 1.7184, Acc: 0.7389\n",
      "Epoch [37/75], Loss: 1.7080, Acc: 0.7409\n",
      "Epoch [38/75], Loss: 1.6971, Acc: 0.7446\n",
      "Epoch [39/75], Loss: 1.6861, Acc: 0.7467\n",
      "Epoch [40/75], Loss: 1.6747, Acc: 0.7467\n",
      "Epoch [41/75], Loss: 1.6634, Acc: 0.7498\n",
      "Epoch [42/75], Loss: 1.6519, Acc: 0.7495\n",
      "Epoch [43/75], Loss: 1.6407, Acc: 0.7503\n",
      "Epoch [44/75], Loss: 1.6281, Acc: 0.7509\n",
      "Epoch [45/75], Loss: 1.6164, Acc: 0.7520\n",
      "Epoch [46/75], Loss: 1.6039, Acc: 0.7520\n",
      "Epoch [47/75], Loss: 1.5916, Acc: 0.7528\n",
      "Epoch [48/75], Loss: 1.5786, Acc: 0.7566\n",
      "Epoch [49/75], Loss: 1.5659, Acc: 0.7559\n",
      "Epoch [50/75], Loss: 1.5537, Acc: 0.7561\n",
      "Epoch [51/75], Loss: 1.5406, Acc: 0.7588\n",
      "Epoch [52/75], Loss: 1.5275, Acc: 0.7569\n",
      "Epoch [53/75], Loss: 1.5139, Acc: 0.7586\n",
      "Epoch [54/75], Loss: 1.5002, Acc: 0.7595\n",
      "Epoch [55/75], Loss: 1.4872, Acc: 0.7610\n",
      "Epoch [56/75], Loss: 1.4730, Acc: 0.7631\n",
      "Epoch [57/75], Loss: 1.4598, Acc: 0.7631\n",
      "Epoch [58/75], Loss: 1.4462, Acc: 0.7656\n",
      "Epoch [59/75], Loss: 1.4308, Acc: 0.7667\n",
      "Epoch [60/75], Loss: 1.4182, Acc: 0.7657\n",
      "Epoch [61/75], Loss: 1.4038, Acc: 0.7681\n",
      "Epoch [62/75], Loss: 1.3899, Acc: 0.7697\n",
      "Epoch [63/75], Loss: 1.3751, Acc: 0.7711\n",
      "Epoch [64/75], Loss: 1.3611, Acc: 0.7713\n",
      "Epoch [65/75], Loss: 1.3485, Acc: 0.7732\n",
      "Epoch [66/75], Loss: 1.3338, Acc: 0.7746\n",
      "Epoch [67/75], Loss: 1.3192, Acc: 0.7773\n",
      "Epoch [68/75], Loss: 1.3044, Acc: 0.7776\n",
      "Epoch [69/75], Loss: 1.2909, Acc: 0.7775\n",
      "Epoch [70/75], Loss: 1.2780, Acc: 0.7785\n",
      "Epoch [71/75], Loss: 1.2643, Acc: 0.7811\n",
      "Epoch [72/75], Loss: 1.2486, Acc: 0.7832\n",
      "Epoch [73/75], Loss: 1.2356, Acc: 0.7823\n",
      "Epoch [74/75], Loss: 1.2226, Acc: 0.7832\n",
      "Epoch [75/75], Loss: 1.2082, Acc: 0.7864\n"
     ]
    }
   ],
   "source": [
    "#設定類別的權重，因為資料集不平衡\n",
    "class_counts = torch.tensor([ 5459,  4018, 12158,  2699,  3710,  9814,  2142],dtype=torch.float32)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()*7\n",
    "#設定參數\n",
    "input_dim = X_train.shape[1]\n",
    "hidden_dim = 128\n",
    "output_dim = y_train.shape[1]\n",
    "#初始化模型\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#訓練模型\n",
    "epochs = 75\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)\n",
    "    y_train_class = y_train.argmax(dim=1)\n",
    "    loss = criterion(outputs, y_train_class)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    with torch.no_grad():\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        acc = accuracy_score(y_train_class.numpy(), preds.numpy())\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}], Loss: {loss.item():.4f}, Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a67e550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.4136, Test Acc: 0.6563\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model(X_test)\n",
    "    y_test_class = y_test.argmax(dim=1)\n",
    "    test_loss = criterion(outputs, y_test_class)\n",
    "    preds = torch.argmax(outputs, dim=1)\n",
    "    test_acc = accuracy_score(y_test_class.numpy(), preds.numpy())\n",
    "    print(f\"Test Loss: {test_loss.item():.4f}, Test Acc: {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb1494",
   "metadata": {},
   "source": [
    "### Data preprocessing(sklearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6cd036ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_cleaned = [preprocess_text(t) for t in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "46ae8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "X = vectorizer.fit_transform(texts_cleaned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "bfe2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "Y = mlb.fit_transform(labels)   # shape = (n_samples, n_emotions)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c72638",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       Anger       0.90      0.72      0.80     11991\n",
      "        Fear       0.88      0.67      0.76      9821\n",
      "         Joy       0.91      0.82      0.86     27407\n",
      "        Love       0.79      0.54      0.64      9132\n",
      "     Neutral       0.61      0.18      0.28      5100\n",
      "     Sadness       0.92      0.86      0.89     22308\n",
      "    Surprise       0.76      0.41      0.53      3885\n",
      "\n",
      "   micro avg       0.89      0.71      0.79     89644\n",
      "   macro avg       0.82      0.60      0.68     89644\n",
      "weighted avg       0.87      0.71      0.78     89644\n",
      " samples avg       0.71      0.73      0.72     89644\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "sklearn_model=LogisticRegression(max_iter=1000)\n",
    "clf= OneVsRestClassifier(sklearn_model)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
