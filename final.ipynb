{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46b546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "import joblib\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51118a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Fenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\Fenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Fenne\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下載 nltk 資源（只需一次）\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0672aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#有GPU用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fa7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = datasets.load_from_disk(\"super-emotion\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts_train  = train_dataset[\"text\"]\n",
    "labels_train = train_dataset[\"labels\"]\n",
    "\n",
    "texts_test = test_dataset[\"text\"]\n",
    "labels_test = test_dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44aa72",
   "metadata": {},
   "source": [
    "### Data Preprocessing(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ce5a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [00:30<00:00, 14.43it/s]\n",
      "100%|██████████| 59/59 [00:04<00:00, 13.48it/s]\n"
     ]
    }
   ],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "def preprocess_and_save(texts, output_file, batch_size=1000):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            cleaned_batch = [preprocess_text(t) for t in batch]\n",
    "            for line in cleaned_batch:\n",
    "                f.write(json.dumps(line) + \"\\n\")\n",
    "preprocess_and_save(texts_train, \"texts_train_cleaned.jsonl\")\n",
    "preprocess_and_save(texts_test, \"texts_test_cleaned.jsonl\")\n",
    "with open(\"labels_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_train, f)\n",
    "with open(\"labels_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_test, f)\n",
    "def load_cleaned_texts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "texts_cleaned_train = load_cleaned_texts(\"texts_train_cleaned.jsonl\")\n",
    "texts_cleaned_test = load_cleaned_texts(\"texts_test_cleaned.jsonl\")\n",
    "#RAM會爆炸!!\n",
    "#texts_cleaned_train = [preprocess_text(t) for t in texts_train [:50000]]\n",
    "#texts_cleaned_test = [preprocess_text(t) for t in texts_test [:50000]]\n",
    "#labels_subset_train = labels_train[:50000]\n",
    "#labels_subset_test = labels_test[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c0553fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 vectorizer，直接使用 float32，減少轉型開銷\n",
    "vectorizer = TfidfVectorizer(max_features=10000, dtype=np.float32)\n",
    "vectorizer.fit(texts_cleaned_train)\n",
    "\n",
    "# 定義記憶體優化版的批次轉換\n",
    "def vectorize_in_batch(texts, batch_size=50):  # 可視情況調整 batch_size\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        # 直接產生 float32 稀疏矩陣，再轉成 dense\n",
    "        X_batch = vectorizer.transform(batch).toarray()  # 已是 float32\n",
    "        yield torch.tensor(X_batch)  # 生成 PyTorch tensor\n",
    "\n",
    "# 使用範例 - 訓練資料\n",
    "X_train_batches = []\n",
    "for X_batch_tensor in vectorize_in_batch(texts_cleaned_train):\n",
    "    X_train_batches.append(X_batch_tensor)\n",
    "\n",
    "# 使用範例 - 測試資料\n",
    "X_test_batches = []\n",
    "for X_batch_tensor in vectorize_in_batch(texts_cleaned_test):\n",
    "    X_test_batches.append(X_batch_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f5a6e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "#mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_subset_train)   # shape = (n_samples, n_emotions)\n",
    "#y_train = mlb.fit_transform(labels_subset_train)\n",
    "#y_test = mlb.transform(labels_subset_test)\n",
    "#y_train = mlb.fit_transform(labels_train)   # shape = (n_samples, n_emotions)\n",
    "#y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "#原本為多標籤，取第一個標籤作為分類\n",
    "y_train_single = [labels[0] for labels in labels_train]\n",
    "y_test_single = [labels[0] for labels in labels_test]\n",
    "#轉成tensor\n",
    "train_data = []\n",
    "start_idx = 0\n",
    "for batch in X_train_batches:\n",
    "    current_batch_size = batch.shape[0]\n",
    "    end_idx = start_idx + current_batch_size\n",
    "    train_data.append((batch, torch.tensor(y_train_single[start_idx:end_idx], dtype=torch.long)))\n",
    "    start_idx = end_idx\n",
    "test_data = []\n",
    "start_idx = 0\n",
    "for batch in X_test_batches:\n",
    "    current_batch_size = batch.shape[0]\n",
    "    end_idx = start_idx + current_batch_size\n",
    "    test_data.append((batch, torch.tensor(y_test_single[start_idx:end_idx], dtype=torch.long)))\n",
    "    start_idx = end_idx\n",
    "#y_train = torch.tensor(y_train_single, dtype=torch.long)\n",
    "#y_test = torch.tensor(y_test_single, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8652a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.net=nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(hidden_dim, hidden_dim//2),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim//2, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "71bb329c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/50], Loss: 0.8502, Acc: 0.7092\n",
      "Epoch [2/50], Loss: 0.4910, Acc: 0.8024\n",
      "Epoch [3/50], Loss: 0.4493, Acc: 0.8108\n",
      "Epoch [4/50], Loss: 0.4229, Acc: 0.8173\n",
      "Epoch [5/50], Loss: 0.4048, Acc: 0.8218\n",
      "Epoch [6/50], Loss: 0.3903, Acc: 0.8256\n",
      "Epoch [7/50], Loss: 0.3781, Acc: 0.8299\n",
      "Epoch [8/50], Loss: 0.3675, Acc: 0.8334\n",
      "Epoch [9/50], Loss: 0.3571, Acc: 0.8373\n",
      "Epoch [10/50], Loss: 0.3476, Acc: 0.8410\n",
      "Epoch [11/50], Loss: 0.3384, Acc: 0.8447\n",
      "Epoch [12/50], Loss: 0.3299, Acc: 0.8485\n",
      "Epoch [13/50], Loss: 0.3219, Acc: 0.8513\n",
      "Epoch [14/50], Loss: 0.3148, Acc: 0.8543\n",
      "Epoch [15/50], Loss: 0.3069, Acc: 0.8573\n",
      "Epoch [16/50], Loss: 0.3002, Acc: 0.8600\n",
      "Epoch [17/50], Loss: 0.2938, Acc: 0.8631\n",
      "Epoch [18/50], Loss: 0.2870, Acc: 0.8656\n",
      "Epoch [19/50], Loss: 0.2814, Acc: 0.8683\n",
      "Epoch [20/50], Loss: 0.2765, Acc: 0.8705\n",
      "Epoch [21/50], Loss: 0.2715, Acc: 0.8727\n",
      "Epoch [22/50], Loss: 0.2660, Acc: 0.8746\n",
      "Epoch [23/50], Loss: 0.2605, Acc: 0.8772\n",
      "Epoch [24/50], Loss: 0.2576, Acc: 0.8780\n",
      "Epoch [25/50], Loss: 0.2524, Acc: 0.8804\n",
      "Epoch [26/50], Loss: 0.2492, Acc: 0.8817\n",
      "Epoch [27/50], Loss: 0.2462, Acc: 0.8836\n",
      "Epoch [28/50], Loss: 0.2430, Acc: 0.8845\n",
      "Epoch [29/50], Loss: 0.2392, Acc: 0.8866\n",
      "Epoch [30/50], Loss: 0.2368, Acc: 0.8869\n",
      "Epoch [31/50], Loss: 0.2337, Acc: 0.8885\n",
      "Epoch [32/50], Loss: 0.2313, Acc: 0.8895\n",
      "Epoch [33/50], Loss: 0.2277, Acc: 0.8910\n",
      "Epoch [34/50], Loss: 0.2262, Acc: 0.8917\n",
      "Epoch [35/50], Loss: 0.2240, Acc: 0.8925\n",
      "Epoch [36/50], Loss: 0.2229, Acc: 0.8940\n",
      "Epoch [37/50], Loss: 0.2211, Acc: 0.8942\n",
      "Epoch [38/50], Loss: 0.2182, Acc: 0.8956\n",
      "Epoch [39/50], Loss: 0.2154, Acc: 0.8966\n",
      "Epoch [40/50], Loss: 0.2148, Acc: 0.8972\n",
      "Epoch [41/50], Loss: 0.2126, Acc: 0.8978\n",
      "Epoch [42/50], Loss: 0.2118, Acc: 0.8985\n",
      "Epoch [43/50], Loss: 0.2095, Acc: 0.8993\n",
      "Epoch [44/50], Loss: 0.2091, Acc: 0.8997\n",
      "Epoch [45/50], Loss: 0.2071, Acc: 0.9008\n",
      "Epoch [46/50], Loss: 0.2055, Acc: 0.9010\n",
      "Epoch [47/50], Loss: 0.2047, Acc: 0.9017\n",
      "Epoch [48/50], Loss: 0.2029, Acc: 0.9023\n",
      "Epoch [49/50], Loss: 0.2023, Acc: 0.9027\n",
      "Epoch [50/50], Loss: 0.2015, Acc: 0.9032\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#設定類別的權重，因為資料集不平衡\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(7),\n",
    "    y=y_train_single\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "#設定參數\n",
    "input_dim = X_train_batches[0].shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = 7\n",
    "#初始化模型\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "#訓練模型\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for X_batch, y_batch in train_data:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y_batch.cpu().tolist())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_data):.4f}, Acc: {acc:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a67e550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.8828, Test Acc: 0.7678\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_data:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "    avg_loss = total_loss / len(test_data)\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50abd784",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tfidf_vectorizer.pkl']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"mlp_weights.pth\")\n",
    "joblib.dump(vectorizer, \"tfidf_vectorizer.pkl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb1494",
   "metadata": {},
   "source": [
    "### Data preprocessing(sklearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6cd036ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_cleaned_train = [preprocess_text(t) for t in texts_train]\n",
    "texts_cleaned_test = [preprocess_text(t) for t in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "# X = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_train = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_test = vectorizer.transform(texts_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_train)   \n",
    "y_train = mlb.fit_transform(labels_train) # shape = (n_samples, n_emotions)\n",
    "y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c72638",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model=LogisticRegression(max_iter=1000)\n",
    "clf= OneVsRestClassifier(sklearn_model)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
