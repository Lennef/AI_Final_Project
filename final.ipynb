{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46b546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c51118a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\jimmy\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 下載 nltk 資源（只需一次）\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0672aa12",
   "metadata": {},
   "outputs": [],
   "source": [
    "#有GPU用GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "35fa7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = datasets.load_from_disk(\"super-emotion\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts_train  = train_dataset[\"text\"]\n",
    "labels_train = train_dataset[\"labels\"]\n",
    "\n",
    "texts_test = test_dataset[\"text\"]\n",
    "labels_test = test_dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44aa72",
   "metadata": {},
   "source": [
    "### Data Preprocessing(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ce5a7ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 440/440 [00:19<00:00, 22.82it/s]\n",
      "100%|██████████| 59/59 [00:02<00:00, 22.59it/s]\n"
     ]
    }
   ],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "def preprocess_and_save(texts, output_file, batch_size=1000):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            cleaned_batch = [preprocess_text(t) for t in batch]\n",
    "            for line in cleaned_batch:\n",
    "                f.write(json.dumps(line) + \"\\n\")\n",
    "preprocess_and_save(texts_train, \"texts_train_cleaned.jsonl\")\n",
    "preprocess_and_save(texts_test, \"texts_test_cleaned.jsonl\")\n",
    "with open(\"labels_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_train, f)\n",
    "with open(\"labels_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_test, f)\n",
    "def load_cleaned_texts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "texts_cleaned_train = load_cleaned_texts(\"texts_train_cleaned.jsonl\")\n",
    "texts_cleaned_test = load_cleaned_texts(\"texts_test_cleaned.jsonl\")\n",
    "#RAM會爆炸!!\n",
    "#texts_cleaned_train = [preprocess_text(t) for t in texts_train [:50000]]\n",
    "#texts_cleaned_test = [preprocess_text(t) for t in texts_test [:50000]]\n",
    "#labels_subset_train = labels_train[:50000]\n",
    "#labels_subset_test = labels_test[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7c0553fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "vectorizer.fit(texts_cleaned_train)\n",
    "def vectorize_in_batch(texts, batch_size=1000):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        X_batch = vectorizer.transform(batch).toarray().astype(np.float32)\n",
    "        vectors.append(torch.tensor(X_batch))\n",
    "    return vectors\n",
    "# X = vectorizer.fit_transform(texts_cleaned_train).toarray().astype(np.float32)\n",
    "X_train_batches = vectorize_in_batch(texts_cleaned_train)\n",
    "X_test_batches = vectorize_in_batch(texts_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f5a6e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "#mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_subset_train)   # shape = (n_samples, n_emotions)\n",
    "#y_train = mlb.fit_transform(labels_subset_train)\n",
    "#y_test = mlb.transform(labels_subset_test)\n",
    "#y_train = mlb.fit_transform(labels_train)   # shape = (n_samples, n_emotions)\n",
    "#y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "#原本為多標籤，取第一個標籤作為分類\n",
    "y_train_single = [labels[0] for labels in labels_train]\n",
    "y_test_single = [labels[0] for labels in labels_test]\n",
    "#轉成tensor\n",
    "train_data = []\n",
    "start_idx = 0\n",
    "for batch in X_train_batches:\n",
    "    current_batch_size = batch.shape[0]\n",
    "    end_idx = start_idx + current_batch_size\n",
    "    train_data.append((batch, torch.tensor(y_train_single[start_idx:end_idx], dtype=torch.long)))\n",
    "    start_idx = end_idx\n",
    "test_data = []\n",
    "start_idx = 0\n",
    "for batch in X_test_batches:\n",
    "    current_batch_size = batch.shape[0]\n",
    "    end_idx = start_idx + current_batch_size\n",
    "    test_data.append((batch, torch.tensor(y_test_single[start_idx:end_idx], dtype=torch.long)))\n",
    "    start_idx = end_idx\n",
    "#y_train = torch.tensor(y_train_single, dtype=torch.long)\n",
    "#y_test = torch.tensor(y_test_single, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8652a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        ).to(device)\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb329c",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA_nll_loss_forward)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[26]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     26\u001b[39m optimizer.zero_grad()\n\u001b[32m     27\u001b[39m outputs = model(X_batch)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m loss = \u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m loss.backward()\n\u001b[32m     30\u001b[39m optimizer.step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\modules\\loss.py:1295\u001b[39m, in \u001b[36mCrossEntropyLoss.forward\u001b[39m\u001b[34m(self, input, target)\u001b[39m\n\u001b[32m   1294\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, target: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1295\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1296\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1297\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1298\u001b[39m \u001b[43m        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1299\u001b[39m \u001b[43m        \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1300\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1301\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1302\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\jimmy\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\torch\\nn\\functional.py:3494\u001b[39m, in \u001b[36mcross_entropy\u001b[39m\u001b[34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[39m\n\u001b[32m   3492\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m size_average \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m reduce \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   3493\u001b[39m     reduction = _Reduction.legacy_get_string(size_average, reduce)\n\u001b[32m-> \u001b[39m\u001b[32m3494\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_nn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcross_entropy_loss\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   3495\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   3496\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3497\u001b[39m \u001b[43m    \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3498\u001b[39m \u001b[43m    \u001b[49m\u001b[43m_Reduction\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_enum\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreduction\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3499\u001b[39m \u001b[43m    \u001b[49m\u001b[43mignore_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3500\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlabel_smoothing\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   3501\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument weight in method wrapper_CUDA_nll_loss_forward)"
     ]
    }
   ],
   "source": [
    "\n",
    "#設定類別的權重，因為資料集不平衡\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight='balanced',\n",
    "    classes=np.arange(7),\n",
    "    y=y_train_single\n",
    ")\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float32)\n",
    "#設定參數\n",
    "input_dim = X_train_batches[0].shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = 7\n",
    "#初始化模型\n",
    "model = MLP(input_dim, hidden_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "#訓練模型\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for X_batch, y_batch in train_data:\n",
    "        X_batch = X_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.cpu().tolist())\n",
    "        all_labels.extend(y_batch.cpu().tolist())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_data):.4f}, Acc: {acc:.4f}\")\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a67e550e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 2.6129, Test Acc: 0.3981\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_data:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "    avg_loss = total_loss / len(test_data)\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb1494",
   "metadata": {},
   "source": [
    "### Data preprocessing(sklearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd036ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_cleaned_train = [preprocess_text(t) for t in texts_train]\n",
    "texts_cleaned_test = [preprocess_text(t) for t in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "# X = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_train = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_test = vectorizer.transform(texts_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_train)   \n",
    "y_train = mlb.fit_transform(labels_train) # shape = (n_samples, n_emotions)\n",
    "y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c72638",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model=LogisticRegression(max_iter=1000)\n",
    "clf= OneVsRestClassifier(sklearn_model)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
