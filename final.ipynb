{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b546d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets\n",
    "import numpy as np\n",
    "import evaluate\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import nltk\n",
    "import string\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.metrics import classification_report,accuracy_score\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c51118a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 下載 nltk 資源（只需一次）\n",
    "nltk.download('punkt')\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35fa7edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load the dataset\n",
    "dataset = datasets.load_from_disk(\"super-emotion\")\n",
    "train_dataset = dataset[\"train\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "texts_train  = train_dataset[\"text\"]\n",
    "labels_train = train_dataset[\"labels\"]\n",
    "\n",
    "texts_test = test_dataset[\"text\"]\n",
    "labels_test = test_dataset[\"labels\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb44aa72",
   "metadata": {},
   "source": [
    "### Data Preprocessing(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ce5a7ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "def preprocess_and_save(texts, output_file, batch_size=1000):\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        for i in tqdm(range(0, len(texts), batch_size)):\n",
    "            batch = texts[i:i + batch_size]\n",
    "            cleaned_batch = [preprocess_text(t) for t in batch]\n",
    "            for line in cleaned_batch:\n",
    "                f.write(json.dumps(line) + \"\\n\")\n",
    "preprocess_and_save(texts_train, \"texts_train_cleaned.jsonl\")\n",
    "preprocess_and_save(texts_test, \"texts_test_cleaned.jsonl\")\n",
    "with open(\"labels_train.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_train, f)\n",
    "with open(\"labels_test.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(labels_test, f)\n",
    "def load_cleaned_texts(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return [json.loads(line) for line in f]\n",
    "texts_cleaned_train = load_cleaned_texts(\"texts_train_cleaned.jsonl\")\n",
    "texts_cleaned_test = load_cleaned_texts(\"texts_test_cleaned.jsonl\")\n",
    "#RAM會爆炸!!\n",
    "#texts_cleaned_train = [preprocess_text(t) for t in texts_train [:50000]]\n",
    "#texts_cleaned_test = [preprocess_text(t) for t in texts_test [:50000]]\n",
    "#labels_subset_train = labels_train[:50000]\n",
    "#labels_subset_test = labels_test[:50000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c0553fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "vectorizer.fit(texts_cleaned_train)\n",
    "def vectorize_in_batch(texts, batch_size=1000):\n",
    "    vectors = []\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i + batch_size]\n",
    "        X_batch = vectorizer.transform(batch).toarray().astype(np.float32)\n",
    "        vectors.append(torch.tensor(X_batch))\n",
    "    return vectors\n",
    "# X = vectorizer.fit_transform(texts_cleaned_train).toarray().astype(np.float32)\n",
    "X_train_batches = vectorize_in_batch(texts_cleaned_train)\n",
    "X_test_batches = vectorize_in_batch(texts_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a6e2a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "#mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_subset_train)   # shape = (n_samples, n_emotions)\n",
    "#y_train = mlb.fit_transform(labels_subset_train)\n",
    "#y_test = mlb.transform(labels_subset_test)\n",
    "#y_train = mlb.fit_transform(labels_train)   # shape = (n_samples, n_emotions)\n",
    "#y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "#X_train = torch.tensor(X_train, dtype=torch.float32)\n",
    "#X_test = torch.tensor(X_test, dtype=torch.float32)\n",
    "#原本為多標籤，取第一個標籤作為分類\n",
    "y_train_single = [labels[0] for labels in labels_train]\n",
    "y_test_single = [labels[0] for labels in labels_test]\n",
    "#轉成tensor\n",
    "train_data = [(batch, torch.tensor(y_train_single[i:i+len(batch)], dtype=torch.long)) for i, batch in enumerate(X_train_batches)]\n",
    "test_data = [(batch, torch.tensor(y_test_single[i:i+len(batch)], dtype=torch.long)) for i, batch in enumerate(X_test_batches)]\n",
    "#y_train = torch.tensor(y_train_single, dtype=torch.long)\n",
    "#y_test = torch.tensor(y_test_single, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8652a652",
   "metadata": {},
   "outputs": [],
   "source": [
    "#建立模型\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(MLP, self).__init__()\n",
    "        self.model=nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(hidden_dim, output_dim)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71bb329c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#設定類別的權重，因為資料集不平衡\n",
    "label_counter = Counter(y_train_single)\n",
    "class_counts = torch.tensor([label_counter[i] for i in range(7)], dtype=torch.float32)\n",
    "class_weights = 1.0 / class_counts\n",
    "class_weights = class_weights / class_weights.sum()*7\n",
    "#設定參數\n",
    "input_dim = X_train_batches[0].shape[1]\n",
    "hidden_dim = 256\n",
    "output_dim = 7\n",
    "#初始化模型\n",
    "model = MLP(input_dim, hidden_dim, output_dim)\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.005)\n",
    "#訓練模型\n",
    "epochs = 75\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    for X_batch, y_batch in train_data:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.extend(preds.tolist())\n",
    "        all_labels.extend(y_batch.tolist())\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Epoch [{epoch+1}/{epochs}], Loss: {total_loss/len(train_data):.4f}, Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a67e550e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "total_loss = 0.0\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_data:\n",
    "        outputs = model(X_batch)\n",
    "        loss = criterion(outputs, y_batch)\n",
    "        total_loss += loss.item()\n",
    "        preds = torch.argmax(outputs, dim=1)\n",
    "        all_preds.append(preds)\n",
    "        all_labels.append(y_batch)\n",
    "    avg_loss = total_loss / len(test_data)\n",
    "    all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "    all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Loss: {avg_loss:.4f}, Test Acc: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbb1494",
   "metadata": {},
   "source": [
    "### Data preprocessing(sklearning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd036ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NLTK prerocessing\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    tokens = word_tokenize(text)\n",
    "    tokens = [t for t in tokens if t.isalpha()]  # 移除標點、數字\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "texts_cleaned_train = [preprocess_text(t) for t in texts_train]\n",
    "texts_cleaned_test = [preprocess_text(t) for t in texts_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46ae8352",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TF-IDF 向量化\n",
    "vectorizer = TfidfVectorizer(max_features=10000)\n",
    "# X = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_train = vectorizer.fit_transform(texts_cleaned_train)\n",
    "X_test = vectorizer.transform(texts_cleaned_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfe2fa81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label multi-hot encoding\n",
    "mlb = MultiLabelBinarizer()\n",
    "# Y = mlb.fit_transform(labels_train)   \n",
    "y_train = mlb.fit_transform(labels_train) # shape = (n_samples, n_emotions)\n",
    "y_test = mlb.transform(labels_test)\n",
    "\n",
    "# 分割訓練集與測試集\n",
    "# X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98c72638",
   "metadata": {},
   "outputs": [],
   "source": [
    "sklearn_model=LogisticRegression(max_iter=1000)\n",
    "clf= OneVsRestClassifier(sklearn_model)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "clf.fit(X_train, y_train)\n",
    "print(classification_report(y_test, y_pred, target_names=mlb.classes_))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
